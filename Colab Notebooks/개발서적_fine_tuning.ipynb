{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"11QpBDwGMOITpJAqwy2MpubvYiKddYN7C","authorship_tag":"ABX9TyO+EsjVtDMlwolYYuZpk5CH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":29,"metadata":{"id":"VvYl16a8LQdV","executionInfo":{"status":"ok","timestamp":1702030683018,"user_tz":-540,"elapsed":354,"user":{"displayName":"차호현","userId":"07716043975857445097"}}},"outputs":[],"source":["import pandas as pd\n","\n","# 대화 데이터 구성\n","dialog_data = [\n","    {\"Role\": \"User\", \"Contents\": \"다음학기에 파이썬을 배우려고 하는데, 어떤 책을 추천해주시겠어요?\"},\n","    {\"Role\": \"Model\", \"Contents\": \"(Do it!) 점프 투 파이썬 책을 추천합니다.\"},\n","    {\"Role\": \"User\", \"Contents\": \"이 책의 KDC 랑 대출건수도 알려줄래요?\"},\n","    {\"Role\": \"Model\", \"Contents\": \"KDC는 005.133이고 대출건수는 147 입니다.\"},\n","    {\"Role\": \"User\", \"Contents\": \"C 언어 에 대해 공부하고 싶은데, 책 좀 추천해줘\"},\n","    {\"Role\": \"Model\", \"Contents\": \"윤성우의 열혈 C 프로그래밍 책을 추천합니다.\"},\n","    {\"Role\": \"User\", \"Contents\": \"이 책의 KDC 랑 대출건수도 알려줄래?\"},\n","    {\"Role\": \"Model\", \"Contents\": \"KDC는 005.135이고 대출건수는 113 입니다.\"},\n","    {\"Role\": \"User\", \"Contents\": \"프로젝트 준비로 알고리즘에 대해서 알고싶은데, 어떤 책이 좋을까?\"},\n","    {\"Role\": \"Model\", \"Contents\": \"알고리즘 행성 : 쇼핑부터 인공지능까지, 우리 삶을 움직이는 알고리즘에 관한 모든것 책을 추천합니다.\"},\n","    {\"Role\": \"User\", \"Contents\": \"이 책의 KDC 랑 대출건수도 알려줘\"},\n","    {\"Role\": \"Model\", \"Contents\": \"KDC는 005.1이고 대출건수는 133 입니다.\"},\n","    {\"Role\": \"User\", \"Contents\": \"파이썬 너무 어려운데, 어떤 책이 좋을까\"},\n","    {\"Role\": \"Model\", \"Contents\": \"(Do it!) 점프 투 파이썬 책을 추천합니다.\"},\n","    {\"Role\": \"User\", \"Contents\": \"이 책의 KDC 랑 대출건수도 알려줄래요?\"},\n","    {\"Role\": \"Model\", \"Contents\": \"KDC는 005.133이고 대출건수는 147 입니다.\"},\n","    {\"Role\": \"User\", \"Contents\": \"운영체제 관련책을 추천해줘\"},\n","    {\"Role\": \"Model\", \"Contents\": \"(지금 당장 써먹는) 윈도우 10 마스터북 : 진화하는 MS운영체제의 끝판왕 책을 추천합니다.\"},\n","    {\"Role\": \"User\", \"Contents\": \"이 책의 KDC 랑 대출건수도 알려줄래요?\"},\n","    {\"Role\": \"Model\", \"Contents\": \"KDC는 005.44이고 대출건수는 23 입니다.\"},\n","]\n","\n","# 대화 데이터를 데이터프레임으로 변환\n","df = pd.DataFrame(dialog_data)\n","\n","# 대화 데이터를 CSV 파일로 저장\n","df.to_csv(\"conversation_dataset.csv\", index=False)\n","\n","# 키워드 데이터를 로드\n","\n","# 열 이름을 리스트로 지정\n","column_names = [\"Keywords\"]\n","\n","# 데이터를 로드할 때 열 이름을 설정\n","df1 = pd.read_excel('/content/drive/MyDrive/대출 데이터 모음/키워드 처리 폴더/keyword.xlsx', names=column_names)\n","\n","# 열 이름이 설정된 데이터프레임을 CSV 파일로 저장\n","df1.to_csv('/keyword_dataset.csv', index=False)\n"]},{"cell_type":"code","source":["import pandas as pd\n","\n","# 대화 데이터 로드\n","conversation_df = pd.read_csv(\"conversation_dataset.csv\")\n","keywords_df = pd.read_csv(\"keywords_dataset.csv\")"],"metadata":{"id":"wY9COTKE2vva","executionInfo":{"status":"ok","timestamp":1702030685479,"user_tz":-540,"elapsed":295,"user":{"displayName":"차호현","userId":"07716043975857445097"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["from transformers import PreTrainedTokenizerFast\n","\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"gpt2\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mJN305G422Ez","executionInfo":{"status":"ok","timestamp":1702030687634,"user_tz":-540,"elapsed":914,"user":{"displayName":"차호현","userId":"07716043975857445097"}},"outputId":"c96adc40-ffe8-4b4d-ee93-7304843bc0b0"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n","The class this function is called from is 'PreTrainedTokenizerFast'.\n"]}]},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer, TrainingArguments, Trainer\n","\n","model_name = \"gpt2\"\n","model = GPT2LMHeadModel.from_pretrained(model_name)\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./model_output\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=3,\n","    per_device_train_batch_size=2,\n","    save_total_limit=2,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=100,\n",")\n"],"metadata":{"id":"mo8lfBSQ255A","executionInfo":{"status":"ok","timestamp":1702030693115,"user_tz":-540,"elapsed":4294,"user":{"displayName":"차호현","userId":"07716043975857445097"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","# tokenizer 초기화\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","\n","# 패딩 토큰 추가\n","tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","\n","# preprocess_data 함수 정의\n","def preprocess_data(data_df):\n","    input_texts = list(data_df[\"Contents\"])\n","    input_ids = tokenizer(input_texts, padding=True, truncation=True, return_tensors=\"pt\").input_ids\n","    return {\"input_ids\": input_ids}\n"],"metadata":{"id":"8A_PgCgO295O","executionInfo":{"status":"ok","timestamp":1702030696286,"user_tz":-540,"elapsed":753,"user":{"displayName":"차호현","userId":"07716043975857445097"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# 열 이름을 \"Keywords\"로 설정하여 데이터프레임 생성\n","keywords_df = pd.read_excel('/content/drive/MyDrive/대출 데이터 모음/키워드 처리 폴더/keyword.xlsx', names=[\"Keywords\"])\n","\n","# 열 이름 확인\n","print(keywords_df.columns)\n","\n","# 열 이름이 설정된 데이터프레임을 CSV 파일로 저장\n","keywords_df.to_csv('keyword_dataset.csv', index=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jfc_Bhfr7EqB","executionInfo":{"status":"ok","timestamp":1702030698514,"user_tz":-540,"elapsed":306,"user":{"displayName":"차호현","userId":"07716043975857445097"}},"outputId":"f46061b3-0293-485c-90b9-c46504c8b139"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['Keywords'], dtype='object')\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# tokenizer와 모델 초기화\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n","\n","# 데이터를 토큰화하고 모델 입력 형식에 맞게 변환\n","def preprocess_data(data_df):\n","    input_texts = list(data_df[\"Contents\"])\n","    input_ids = tokenizer(input_texts, padding=True, truncation=True, return_tensors=\"pt\").input_ids\n","    return {\"input_ids\": input_ids}\n","\n","# 대화 데이터를 preprocess\n","train_dataset = preprocess_data(conversation_df)\n","\n","# 추천 키워드 데이터를 로드\n","keywords_df = pd.read_csv(\"keyword_dataset.csv\")\n","\n","# 대화 데이터와 키워드 데이터의 적절한 매핑을 만듭니다.\n","# 여기서는 대화 데이터의 인덱스를 키워드 데이터에 매핑하는 예제를 제공합니다.\n","def preprocess_keywords(data_df):\n","    label_texts = list(data_df[\"Keywords\"])\n","    return {\"labels\": label_texts}\n","\n","# 키워드 데이터를 preprocess\n","test_dataset = preprocess_keywords(keywords_df)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":442},"id":"WM3PWWae4c4G","executionInfo":{"status":"error","timestamp":1702030752672,"user_tz":-540,"elapsed":7130,"user":{"displayName":"차호현","userId":"07716043975857445097"}},"outputId":"79c8dd91-4135-41b5-d8f0-eab4cc8216b9"},"execution_count":36,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-2a0297cb72fe>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 대화 데이터를 preprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconversation_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 추천 키워드 데이터를 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-36-2a0297cb72fe>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(data_df)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0minput_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Contents\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2796\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2798\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2800\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2882\u001b[0m                 )\n\u001b[1;32m   2883\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2884\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2885\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2886\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m         \u001b[0;31m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m         padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n\u001b[0m\u001b[1;32m   3067\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m             \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m         \u001b[0;31m# Test if we have a padding token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpadding_strategy\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDO_NOT_PAD\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2704\u001b[0m                 \u001b[0;34m\"Asking to pad but the tokenizer does not have a padding token. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m                 \u001b[0;34m\"Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."]}]}]}